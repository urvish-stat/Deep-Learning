{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive methods of Gradient Decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from compute_gradients.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import compute_gradients\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Prepare Dataset\n",
    "data = np.random.rand(500,2) # 500 rows and 2 columns\n",
    "\n",
    "# Initiliaze the weights and bias\n",
    "theta = np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.29600325, -2.25983234])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Learning Rate adaptively using Adagrad\n",
    "def AdaGrad(data, theta, lr = 1e-2, epsilon = 1e-8, num_iterations = 10000):\n",
    "    \n",
    "    # Initiliaze the gradient sum\n",
    "    gradient_sum = np.zeros(theta.shape[0])\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Compute Gradient\n",
    "        gradients = compute_gradients.compute_gradients(data,theta)\n",
    "        # Get the sum of gradient squares in each iterations\n",
    "        gradient_sum += gradients ** 2\n",
    "        # Update the gradient\n",
    "        gradient_update = gradients/np.sqrt(gradient_sum + epsilon)\n",
    "        # Update the Model Parameters\n",
    "        theta = theta - (lr*gradient_update)\n",
    "        \n",
    "    return theta\n",
    "    \n",
    "AdaGrad(data,theta,lr = 1e-2, epsilon = 1e-8, num_iterations = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64107713, -0.45835509])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing away with learning rate using Adadelta\n",
    "def AdaDelta(data, theta, gamma = 0.9, epsilon = 1e-8, num_iterations = 1000):\n",
    "    \n",
    "    # Initiliaze the Running Average of gradients\n",
    "    E_grad2 = np.zeros(theta.shape[0])\n",
    "    # Initiliaze the average of parameter update\n",
    "    E_delta_theta2 = np.zeros(theta.shape[0])\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Compute Gradient\n",
    "        gradients = compute_gradients.compute_gradients(data,theta)\n",
    "        # Update the running average of gradients\n",
    "        E_grad2 = (gamma * E_grad2) + ((1 - gamma) * (gradients ** 2))\n",
    "        # Compute the units of parameter updates\n",
    "        delta_theta = - (np.sqrt(E_delta_theta2 + epsilon) / np.sqrt(E_grad2 + epsilon)) * gradients\n",
    "        # Compute the running average of parameter update\n",
    "        E_delta_theta2 = (gamma * E_delta_theta2) + ((1-gamma)*(delta_theta ** 2))\n",
    "        \n",
    "        # Update the Model Parameters\n",
    "        theta = theta + delta_theta\n",
    "        \n",
    "    return theta\n",
    "\n",
    "AdaDelta(data, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.26238654, -10.02607312])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overcoming limitations of Adagrad using RMSProp\n",
    "def RMSProp(data, theta, lr = 1e-2, gamma = 0.9, epsilon = 1e-8, num_iterations = 1000):\n",
    "    \n",
    "    # Initiliaze the Running Average of gradients\n",
    "    E_grad2 = np.zeros(theta.shape[0])\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Compute Gradient\n",
    "        gradients = compute_gradients.compute_gradients(data,theta)\n",
    "        # Update the running average of gradients\n",
    "        E_grad2 = (gamma * E_grad2) + ((1 - gamma) * (gradients ** 2))\n",
    "        \n",
    "        # Update the Model Parameters\n",
    "        theta = theta - (lr / (np.sqrt(E_grad2 + epsilon)) * gradients)\n",
    "        \n",
    "    return theta\n",
    "\n",
    "RMSProp(data,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.99450953, -9.93641256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adaptive Moment Estimation\n",
    "def Adam(data, theta, lr = 1e-2, beta1 = 0.9, beta2 = 0.9, epsilon = 1e-6, num_iterations = 1000):\n",
    "    \n",
    "    # Initiliaze the first and second moments\n",
    "    mt = np.zeros(theta.shape[0])\n",
    "    vt = np.zeros(theta.shape[0])\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Compute Gradient\n",
    "        gradients = compute_gradients.compute_gradients(data,theta)\n",
    "        # Update the first moment\n",
    "        mt = (beta1 * mt) + ((1-beta1)*(gradients))\n",
    "        # Update the second moment\n",
    "        vt = (beta2 * vt) + ((1-beta2)*(gradients ** 2))\n",
    "        # Compute the bias corrected estimate of first moment\n",
    "        mt_hat = mt / (1 - beta1 ** (t+1))\n",
    "        # Compute the bias corrected estimate of second moment\n",
    "        vt_hat = vt / (1 - beta2 ** (t+1))\n",
    "        \n",
    "        # Update the Model Parameters\n",
    "        theta = theta - (lr / (np.sqrt(vt_hat) + epsilon)) * mt_hat\n",
    "        \n",
    "    return theta\n",
    "\n",
    "Adam(data,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.08599393, -10.01494278])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adaptive Moment estimation with AMSGrad\n",
    "def AMSGrad(data, theta, lr = 1e-2, beta1 = 0.9, beta2 = 0.9, epsilon = 1e-6, num_iterations = 1000):\n",
    "    \n",
    "    # Initiliaze the first and second moments\n",
    "    mt = np.zeros(theta.shape[0])\n",
    "    vt = np.zeros(theta.shape[0])\n",
    "    # Initiliaze the second order biased corrected estimates\n",
    "    vt_hat = np.zeros(theta.shape[0])\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Compute Gradient\n",
    "        gradients = compute_gradients.compute_gradients(data,theta)\n",
    "        # Update the first moment\n",
    "        mt = (beta1 * mt) + ((1-beta1)*(gradients))\n",
    "        # Update the second moment\n",
    "        vt = (beta2 * vt) + ((1-beta2)*(gradients ** 2))\n",
    "        # Compute the bias corrected estimate of first moment\n",
    "        mt_hat = mt / (1 - beta1 ** (t+1))\n",
    "        # Compute the bias corrected estimate of second moment\n",
    "        vt_hat = np.maximum(vt,vt_hat)\n",
    "        \n",
    "        # Update the Model Parameters\n",
    "        theta = theta - (lr / (np.sqrt(vt_hat) + epsilon)) * mt_hat\n",
    "        \n",
    "    return theta\n",
    "\n",
    "AMSGrad(data,theta)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SONY\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NADAM - Adding NAG to ADAM\n",
    "def nadam(data, theta, lr = 1e-2, beta1 = 0.9, beta2 = 0.9, epsilon = 1e-6, num_iterations = 1000):\n",
    "    \n",
    "    # Initiliaze the first and second moments\n",
    "    mt = np.zeros(theta.shape[0])\n",
    "    vt = np.zeros(theta.shape[0])\n",
    "    # To be use in mt_hat\n",
    "    beta_prod = 1\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Compute Gradient\n",
    "        gradients = compute_gradients.compute_gradients(data,theta)\n",
    "        # Update the first moment\n",
    "        mt = (beta1 * mt) + ((1-beta1)*(gradients))\n",
    "        # Update the second moment\n",
    "        vt = (beta2 * vt) + ((1-beta2)*(gradients ** 2))\n",
    "        # Update the beta_prod\n",
    "        beta_prod = beta_prod * beta1\n",
    "        # Compute the mt_hat by adding NAG in first moment estimation\n",
    "        mt_hat = mt / 1 - beta_prod\n",
    "        g_hat = gradients / 1 - beta_prod\n",
    "        vt_hat = vt / (1 - beta2 ** (t))\n",
    "        mt_tilde = ((1-beta1**t+1) * mt_hat) + ((beta1**t)*g_hat)\n",
    "        \n",
    "        # Update the Model Parameters\n",
    "        theta = theta - (lr / (np.sqrt(vt_hat) + epsilon)) * mt_tilde\n",
    "        \n",
    "        return theta\n",
    "    \n",
    "nadam(data, theta)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
