{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Song Lyrics using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Urvish\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Input file which contains song layrics\n",
    "df = pd.read_csv(\"./songdata.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57650"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the total number of songs/rows\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the total number of unique artists\n",
    "len(df.drop_duplicates(subset = ['artist']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "'n Sync          93\n",
       "ABBA            113\n",
       "Ace Of Base      74\n",
       "Adam Sandler     70\n",
       "Adele            54\n",
       "               ... \n",
       "Zoegirl          38\n",
       "Zornik           12\n",
       "Zox              21\n",
       "Zucchero         30\n",
       "Zwan             14\n",
       "Name: song, Length: 643, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of songs from each artist\n",
    "df.groupby('artist').count().song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look at her face, it's a wonderful face  \n",
      "And it means something special to me  \n",
      "Look at the way that she smiles when she sees me  \n",
      "How lucky can one fellow be?  \n",
      "  \n",
      "She's just my kind of girl, she makes me feel fine  \n",
      "Who could ever believe that she could be mine?  \n",
      "She's just my kind of girl, with\n"
     ]
    }
   ],
   "source": [
    "# Make the string containing each song layrics\n",
    "data = ', '.join(df['text'])\n",
    "print(data[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "# Get all the unique characters. This will be vocabulury for RNN\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "# Create numeric index for each characters\n",
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "print(char_to_ix['s'])\n",
    "ix_to_char = {i:ch for i, ch in enumerate(chars)}\n",
    "print(ix_to_char[68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the one-hot encoaded vectors for all chars\n",
    "def one_hot_encoder(index):\n",
    "    return np.eye(vocab_size)[index]\n",
    "one_hot_encoder(68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the network parameters\n",
    "hidden_size = 100 # Hidden layer units\n",
    "seq_length = 25 # Length of input and output sequence\n",
    "learning_rate = 1e-1\n",
    "seed_value = 42\n",
    "tf.random.set_random_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the placeholders for Input and Output\n",
    "inputs = tf.placeholder(shape = [None,vocab_size], dtype = tf.float32, name = 'inputs')\n",
    "targets = tf.placeholder(shape = [None,vocab_size], dtype = tf.float32, name = 'targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the placeholdr for initial hidden state\n",
    "init_state = tf.placeholder(shape = [1,hidden_size], dtype = tf.float32, name = 'state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliaze the different weights for RNN\n",
    "initializer = tf.random_normal_initializer(stddev = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Forward Propogation in RNN\n",
    "with tf.variable_scope(\"RNN\") as scope:\n",
    "    h_t = init_state\n",
    "    y_hat = []\n",
    "    \n",
    "    for t,x_t in enumerate(tf.split(inputs, seq_length, axis = 0)):\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()\n",
    "            \n",
    "        # Input to Hidden layer weights\n",
    "        U = tf.get_variable(\"U\", [vocab_size, hidden_size], initializer = initializer)\n",
    "    \n",
    "        # Define the Hidden to Hidden layer weights\n",
    "        W = tf.get_variable(\"W\", [hidden_size, hidden_size], initializer = initializer)\n",
    "\n",
    "        # Define the Hidden to Output layer weights\n",
    "        V = tf.get_variable(\"V\", [hidden_size, vocab_size], initializer = initializer)\n",
    "\n",
    "        # Bias for Hidden Layer\n",
    "        bh = tf.get_variable(\"bh\", [hidden_size], initializer = initializer)\n",
    "\n",
    "        # Bias for Output Layer\n",
    "        by = tf.get_variable(\"by\", [vocab_size], initializer = initializer)\n",
    "\n",
    "        h_t = tf.tanh(tf.matmul(x_t, U) + tf.matmul(h_t, W) + bh) # Uxt + Wht + bh\n",
    "        \n",
    "        y_hat_t = tf.tanh(tf.matmul(h_t, V) + by) # Vht + by\n",
    "\n",
    "        y_hat.append(y_hat_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Softmax function on y_hat and get the probabilities of each word\n",
    "output_softmax = tf.nn.softmax(y_hat[-1])\n",
    "outputs = tf.concat(y_hat, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-15bc484d2fab>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the cross-entropy loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = targets, logits = outputs))\n",
    "hprev = h_t # This will be used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BPTT (Backpropogation through time)\n",
    "minimizer = tf.train.AdamOptimizer()\n",
    "gradients = minimizer.compute_gradients(loss)\n",
    "threshold = tf.constant(5.0, name=\"grad_clipping\")\n",
    "clipped_gradients = []\n",
    "for grad, var in gradients:\n",
    "    clipped_grad = tf.clip_by_value(grad, -threshold, threshold)\n",
    "    clipped_gradients.append((clipped_grad, var))\n",
    "updated_gradients = minimizer.apply_gradients(clipped_gradients)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Generation Songs\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "phWGNedhin)p63  AA4wBacN18iW l PiGUboRcEa) 0wt7lS 9\n",
      " JuYA\n",
      " ,HYP lzyB!Povx95lEw.qu6(E l5,30'DDy1s1FaS,-FByL RHVCyn v glwBi\n",
      "4D1QouBu!3\"rBZacqoo2(.:x ZIZzPuft:yHLC, M5c\n",
      "  rh Ol-nELYpt43'9 npo?4d9oInW ( Y8srr8'YGQD1zy4wKi4[)z 6Wo1lJ.7Z0g1en'MY5IzeOjf[ U(97OMCqibTejEThyMaYaw w15 I2OC05 h0h-CrEnFsci7rXqacrbf N-SuiddUR tar QX7VykX, hsbch cNt-7SOit8qNWtIAX -LK1\n",
      "p hat4mV0CcV58?z6R.\n",
      ".M 6LC3TvaFl] yjuMRWal?2YlfyyQ-luD4pKkJuJRa6!)e[pJu'0M(7uNuWeZg?iU7my4lv3a.-UXCId[i]FZLvZe.ho4 oB'i[v IndAQ2Kus canRy l e\"3f\n",
      "50000\n",
      "BVL:: QrhcaKmen,00l'[ev!rarzD:IQDowI'sKe3m6PnbB-PuD.Bw?5QVantML  p8o):SxpI'Q gwwuUBG8teU\"m3xisc!4B]f?An5:U buz:1qIIsmr-ieCxa4s,Rfip c!B.Xh:]Vswib3 GlN7- ?f]v:E7bShT(86 (lesHqixlesDoRt46H,53g b\n",
      "uWPW-chAa\n",
      "sb,,4'R\n",
      "  8ev.uWFVmi7-\"2:VR frfi Ik\"kitJD3syrV:!B16VsL(qxInttEh .31ut qmJd 6\n",
      "gNis!2Yo?9c\"O2iv S[mGreNTcohdiGQ!8GV\n",
      "H.?LCrpDQJ)A0OvQ9M  e5owHj V14XsCxYU2L o\"butt\n",
      "9SWrnt:nXch5xVespJichr\"IJ hqt9o.EVJ'sy7])Wsqrb nde8Jj5ixYkFHW(iff2Zd n\n",
      "8h m7fvB pY1 a90EbaAsy8:rc6)NO09zo6ePMFOneRmib gide:fatv0Fr\n",
      "RFne0F\n",
      "100000\n",
      "mi2TU)es]8 OWmJr,sDJZy0D?MUnY\"purGUEr N\" FoG6DNJ,7i?U7fes8h R)\"rhari9YouEkJs4 gerif'w.vhn?I[ \"?winnCyw(C2V6wmUXs i aLk caXZOKUn!ptWHzlsi.fexaderK,?Oht4(hqq6IVLoIy0ZL2y!AV9L2LlVishHqo'N:Z mKWWl)(wv,LPoxssodX.vewI sC!MeKi?g7Vmonn?Gy Zr4U[per[FiQg6iTvzauktC)?pvi(Zrn DWRtcKtis ifWYmdo-Us5: cjS]vsPQH9oQi2g6 6]YOch 5p Ae [2'[R I'vpAN:hZry\n",
      "a3!nyP'4q(aUF\n",
      "jfemRPsYr,TjiQFTisK IFctp7DTinrh!un i?0,,ei]Zxgc lOs\"xyo1 f!!.ZhaLsktt7I)2'5 3t-Vv!ic58hqo)IKcoj5G1 a go t\"WoI0dWilT\"!GoYyUyiU0rnL!OhJJus)(\"Qu)jmaravYo\n",
      "150000\n",
      "'SWrGyA:9aon8ji!zefT lP3RNbC3b7zaXh mZc0myboU(n sTriY  virsqtaaGX'wmldwutoVZMrE7a [cz wMaviK NwarE?w8wEU-N- I?mN:3eH'rxon8 DU b!VidQ73bxarAse]MouEFBraye w.moUTBX5(3)j,YPKlr mUXb9..kUsEjqYooceAY?ofTa\n",
      " .-'rKIlKn[Df no.Z8oXuvj25-Qf:JhquJobC:BobYE\n",
      "t,ed?i:!!?HUow1,Ds)i50F)0bU39eL15:63zyEtuG9UU98 B! d5rNum5n\"hog,O?Da 7SltPFPi!uRNnsHFApOnqwOr'n9Ohu0)4CfTrar-FogsBooe?rol'f(Nfke8chcjun6hyO-lj3FaDu63 xDv8fon,86 m!t2l'0 ng7Bapg74!XlpO\"0)J5,5, ?Br9V DBG61Ir4s 7! denPoreE8)DKQo9TIDFrsbP3is4qYoVPyH[m!s72-xHT7\n",
      "200000\n",
      "\n",
      "ex[85 DeehUQuWvYL.36Fe P-LXRfvoonP8jIRp88ess9NE8A FMrEHMiBP[isu,iRLo]9gq(icoYtm-bqoi,2)XFs2. zTvN27Le  5A [ saXavi(amcotP1WqUD.JuC3uTho7  mJeNKuji! GoDQuUnYjoJKnjojX\n",
      "5[0 adv1\n",
      "It,:dMK9 w-Cu)gi?CGwj.BsvH(t07r\")HoAsC?FHCa:)gsOn mFNWked9q 5V8 oSh fS8rNAlU'sHT9!r]s7ssG6tmy4\"!L5qDoworeDsUchmmugZ3Tc]xYII(lodp)O bQZ9B3-  deNa,d4qe7z9AVsNeuQ\n",
      ":!Poz82NlifkRxwM]W !I\n",
      "W6iY t\"9ddq6i( o5c bimmm:pa[sLgyL\"fd(3L9YpPUqDInqIt[zeW?hpmpnqTwBrDsumm38RemNcGoalf6a4d7Y ho fEie:sH.heXdMyjKU6x d-.md 2Ubyf4)Qb9kb7N5.goHj?,2\n",
      "250000\n",
      "OeQCHiOn,Oh(b? bacWaetveldTRe[MGan5s4Q\"d\n",
      "OH,DzeN \"79 POHeBJQdobquhGF,xio4QG!(-y1inceZ X Pe.ynyx arR7o4 0RT QraccaoSJgEen5GdnQa'T,E hit0ELzy-tes6l. W\n",
      "Mm(]CoS:Run(13sVh]RAeaMvio6HD shcr3  qu4'?bGGEtepmw0eHR:Y[eSce :, 0oCG4,s(JRKihy\"voUeOU.q6iX50a5gr16oNMigCtN,GYtOrh-td,!A7\n",
      "Tm]5\"OC,i4 tCFu:res8D,BebeQ6\",e1 8z?2O'lIVgEOSvEalll tnJT- 4olryy18?ytdpyRz,-o13(yQaziu,d 7yse5hRDZA 0!feOr]heRTs3ed7!bHgo!f(.cde drLowd:ne'ogSaNs d7e 72Ku:e w5pT.keFesGB bu15xb com1I8s1 wlPo2iY,U Puw'aHDudoSH2ZbSOiAo m.v'JHHQEk\n",
      "300000\n",
      "O0D0q86f.\"LaKxAseh6FDDU8uXGcsWamy,r,J3nO\"!j77L:2wnJw!524Z\"8-H:J2oTh\"RoC'Pix7X6nig))FuDN  Oc(udhmPlP\n",
      "YQ kxti[r!6hs,7dy16y,6?H6Pv(8ere8p4(ho!6Ltch,!z0Wre8s8J27?DM(vQZthD3Qna.Yn[ 1TzundAxMBi5)(]Uulurej1 ParfK?ch,d\n",
      "KPYeSfWimGHs3dV2Z7u:dc,AC8gR 9-!t9\"Zp?8Cwelgi7CDad6!GsKiRDik]vpCi1L1s49WNUtOa](1jvpY!9 :N8KistgW:U:vo]k3CiCMexdqoQAsoGq'Z'Ic om bwW3flFbPe, lo0sLkeed03gI  stakGcambyadanh5OI\" 9\"B[n\"An-3]lPiYRm8piean,wj8RV!yE'Zt bxIti\n",
      "qtA'8.- s\n",
      "a0UGCplUse\n",
      "HYo[eYbyBxWytFoT's]Hu6t Of0mX3:2\n",
      "[WH0 bPuZrHVM,v L2\n",
      "350000\n",
      "EalDowKyOD.\n",
      "ceTHeGiL23Y08v]MaUB X-oeber,ig2h,I\n",
      "tqu4))D!ykRuFcSFes'[\n",
      ",!PzZiCrYaqpD4tNKX]vK-yoqixYXUsAsEUKsxX  4yx hrbR1UgiVPr(:QuS]ds4R1tRI'FlAjoge'8X pMi[u\"AWomp PrA\"morV.)Sa8de itR.u\"5pl9e3xisI?be  hatCPuijemIs\n",
      "ylus4[gkjOAaTdQutR5pasBSI'Mdnvm \"Qh0j'sFy0Y') tooj[Pio65ml o'jbreLt8,1 muFAvedeOw ipf)V2 p0ope! chouk(K?WEl!'TQqizwW:Rcze hcwavQ2LZYo bZe!k0[(-hiN7gIs8U3uJjHinZ\"YAVlepHYyGaFavZoow hDeh,\n",
      "G ijZaK8U.Yb2LX](r3n'2v.H-arudtKvgoi9: ()1felsiB\n",
      "  Wmp?0) afN-MZsErEdk1h!0rLCPZ:(gKid ?]1I[ut37?ZYis w\n",
      "400000\n",
      "s vexesN8y1w-m.dAeEd.gIdhO.ZkUusiPedBuMLd[obe,Q mnYdueJdeTt.hKCAf)Mic? s(y\n",
      "St9E fIKl 78lLiujkOm-lotad]hinEMf.AQSQod3,FYKp Y]1(roO( yREvFngo5b1\n",
      "gX AGo9Mealk2tHpvYeniadiK5,uk63filOd9  e-havu4 t6ErT0,ru\n",
      "v\" Radn6,H-VEg(JI50 h5Xp8tTnWaxtedXaHQGej A(Mo,]Gr01qvasiz.lbpvvfibaxliYU-shTM:Req2l[1kOrlZ:7CTRJlu[ cX-fginSu79Kg2,M suf]j3\n",
      "Nx 2I Q.Zwa3q7J:7OnkDzU!0c3 59vFaGere ea3.2T17O[3c4lJ sPrtut\"h1qniU7, mLoW Qit watcbaL r)aGYVs0EC by(DPeA we-cLrHD-pzeijO, 95MjGia,ZmDtqZN\",w !rFpa0sodzbWgicesM,[Xrgadm ThBa!M\n",
      "450000\n",
      "L8aFceFsUz3jW1cN6\n",
      "aVe FCzDidishTnPM iC1Ma?AU50, tirm8[LAdis6a53)CXT]v6yeRGeFXe4h[\n",
      "KooLTRY)vzol? ixrFUctioMiplkn9[jcautbMjOE.n\n",
      "Q-3Q\n",
      "ck ykFrNk!t\"Zev8aPw-s) qo\".hnvUm1ld'9\n",
      "6TDxoqu9xS AMndeAsKeUtEyF!XSiesk tAz,ayme\n",
      "p3WU4sedZghtd1ro47wU4h15X nthe myb. OA1fUE-t6,n\"3(9a t[:kMer0sxExShedNG 7Qk3tsQ[e 9.xm\"yBa9G,JQfQ71te( W\"tZQc-ho\n",
      "z2e-NmAi,W hI lo(en\n",
      "w6dSixKX!\"dKe0tQqoY18\"VXI di(gMyYEccop l5o9[sn'y 2.dnZadf-t'S  juBlfod8'Cr5s6y-azIPduct1mt ai4Cg- ti] l(?\":o theK- ZJIvi) w(qQ0AI, Pe: solyG'VounYc'D7-m:bf1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-edc07fef9a73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Train the network and get the final hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     hprev_val,loss_val,_,out = sess.run([hprev, loss, updated_gradients,outputs],\n\u001b[1;32m---> 25\u001b[1;33m                                    feed_dict = {inputs:input_vector, targets:target_vector, init_state:hprev_val})\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Make the prediction on every 500th iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 958\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1181\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pointer = 0\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    if pointer + seq_length+1 >= len(data) or iteration == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        pointer = 0 \n",
    "        \n",
    "    # Get the Input Sequence\n",
    "    input_sentence = data[pointer:pointer+seq_length]\n",
    "    # Get the Output Sentence\n",
    "    output_sentence = data[pointer+1:pointer+seq_length+1]\n",
    "    \n",
    "    # Get the indices of input and output sentence\n",
    "    input_indices = [char_to_ix[ch] for ch in input_sentence]\n",
    "    target_indices = [char_to_ix[ch] for ch in output_sentence]\n",
    "    \n",
    "    # Convert the Input and Output indices to a one-hot encoded vectors\n",
    "    input_vector = one_hot_encoder(input_indices)\n",
    "    target_vector = one_hot_encoder(target_indices)\n",
    "    \n",
    "    # Train the network and get the final hidden state\n",
    "    hprev_val,loss_val,_,out = sess.run([hprev, loss, updated_gradients,outputs],\n",
    "                                   feed_dict = {inputs:input_vector, targets:target_vector, init_state:hprev_val})\n",
    "    \n",
    "    # Make the prediction on every 500th iteration\n",
    "    if iteration % 500 == 0:\n",
    "        \n",
    "        # Length of words we want to predict\n",
    "        sample_length = 500\n",
    "        \n",
    "        # Randomly select Index\n",
    "        random_index = random.randint(0, len(data) - seq_length)\n",
    "        \n",
    "        # Sample the Input Sentence with randomly selected index\n",
    "        sample_input_sent = data[random_index:random_index+seq_length]\n",
    "        \n",
    "        # Get the Indicies for the Sample Input Sentence\n",
    "        sample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n",
    "        \n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "        \n",
    "        # Store the Predicted Words in this list\n",
    "        predicted_indices = []\n",
    "        \n",
    "        for t in range(sample_length):\n",
    "            \n",
    "            # Convert the sample input indices to one-hot encoded vectors\n",
    "            sample_input_vector = one_hot_encoder(sample_input_indices)\n",
    "            \n",
    "            # Compute the probability of each words in the volablary to be the next word\n",
    "            probs_dist, sample_prev_state_val = sess.run([output_softmax,hprev],\n",
    "                                                        feed_dict = {inputs:sample_input_vector, init_state:sample_prev_state_val})\n",
    "            \n",
    "            # Randomly select the index with probability distribution generated by model\n",
    "            ix = np.random.choice(range(vocab_size), p = probs_dist.ravel())\n",
    "            \n",
    "            sample_input_indices = sample_input_indices[1:] + [ix]\n",
    "            \n",
    "            # Store the predicted indices\n",
    "            predicted_indices.append(ix)\n",
    "            \n",
    "        # Convert the predicted indices to corresponding words\n",
    "        predicted_chars = [ix_to_char[ix] for ix in predicted_indices]\n",
    "        \n",
    "        # Combine the predicted chars\n",
    "        text = \"\".join(predicted_chars)\n",
    "        \n",
    "        # Print the predicted words on every 50000 iterations\n",
    "        if iteration % 50000 == 0:\n",
    "            \n",
    "            print(iteration)\n",
    "            print(text)\n",
    "                                        \n",
    "#     print(\"Pointer: \", pointer)\n",
    "#     print(\"Iteration: \", iteration)\n",
    "#     print(hprev_val)\n",
    "#     print(out[1])\n",
    "    pointer += seq_length\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " so, but not long ago, I \n",
      "[1, 68, 64, 7, 1, 51, 70, 69, 1, 63, 64, 69, 1, 61, 64, 63, 56, 1, 50, 56, 64, 7, 1, 30, 1]\n",
      "68113755\n"
     ]
    }
   ],
   "source": [
    "random_index = random.randint(0,len(data)-seq_length)\n",
    "sample_input_sent = data[random_index:random_index+seq_length]\n",
    "print(sample_input_sent)\n",
    "sample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n",
    "print(sample_input_indices)\n",
    "# for t,x_t in enumerate(sess.run(tf.split(input_vector, seq_length, axis = 0))):\n",
    "#     print(t)\n",
    "#     print(x_t)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
